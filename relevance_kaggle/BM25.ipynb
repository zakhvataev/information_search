{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import _pickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import pymorphy2\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stop_set():\n",
    "    s = set()\n",
    "    with open('./stop/stopwords.txt', 'r') as f:\n",
    "        line = f.readline()[:-1]\n",
    "        while line: \n",
    "            s.add(line)\n",
    "            line = f.readline()[:-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_doc():\n",
    "    with open ('urls.numerate.txt') as f:\n",
    "        line = True\n",
    "        u_d = {}\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                id_url = line.split()\n",
    "                u_d[id_url[1]] = id_url[0]\n",
    "    return u_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Получаем словарь URL <-> DOCID\n",
    "\n",
    "\"\"\"\n",
    "URL_DOC = url_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph=pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Сохранение/открытие словарей\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def save_dict(dict_, docid):\n",
    "    with open('./docid_dicts/'+docid+'.pkl', 'wb') as out:\n",
    "        pickle.dump(dict_, out, 2)\n",
    "        \n",
    "def open_dict(docid):\n",
    "    with open('./docid_dicts/'+docid+'.pkl', 'rb') as inp:\n",
    "        d = pickle.load(inp)\n",
    "    return d\n",
    "\n",
    "def open_public(docid):\n",
    "    with open('./dicts_for_spell/'+docid+'.pkl', 'rb') as inp:\n",
    "        d = pickle.load(inp)\n",
    "    return d\n",
    "\n",
    "def open_bigramm(docid):\n",
    "    with open('./bigramms_for_spell/'+docid+'.pkl', 'rb') as inp:\n",
    "        d = pickle.load(inp)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Функция проходит по всем папкам с документами и открывает каждый\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def open_files():\n",
    "    folders = ['./content/content/20170702/', './content/content/20170704/', './content/content/20170707/',\\\n",
    "               './content/content/20170708/', './content/content/20170710/', './content/content/20170711/',\\\n",
    "               './content/content/20170717/', './content/content/20170726/']\n",
    "    i = 0\n",
    "    for cur_folder in folders:\n",
    "        for file in os.listdir(cur_folder):\n",
    "            if i % 100 == 0:\n",
    "                print('FILES LOADED: ', i)\n",
    "            parser(cur_folder+file)\n",
    "            i+=1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Здесь происходит парсинг документа, лежащего по заданному пути\n",
    "\n",
    "Полцчаем URL из первой строки, сопоставляем его с DOCID при помощи словаря URL <-> DOCID\n",
    "\n",
    "При помощи bs4 находим содержимое тегов, чистим при помощи regexp от мусора\n",
    "\n",
    "Производим лемматизацию слов документа, сохраняем в словарь вида: TAG <-> WORD <-> FREQ\n",
    "\n",
    "Что важно - мы сохраняем словарь в виде DOCID.pkl, благодаря этому легко сможем\n",
    "получить доступ к интересующему нас словарю по DOCID\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def parser(way):\n",
    "    \n",
    "    dict_ = {}\n",
    "    \n",
    "    with open(way, encoding='utf-8', errors='ignore') as file:\n",
    "        url = file.readline()[:-1]\n",
    "        \n",
    "        raw_html = file.read()\n",
    "        soup = BeautifulSoup(raw_html, 'html')\n",
    "        \n",
    "        for obj in soup.find_all(['body', 'title', 'h1', 'h2', 'a', 'meta']):\n",
    "            \n",
    "            tag = obj.name\n",
    "            \n",
    "            text = obj.text\n",
    "            text = re.sub('\\W+', ' ', text.lower())\n",
    "            words_list = text.split()\n",
    "            \n",
    "            if tag == 'meta':   \n",
    "                if obj.get('content') != None:\n",
    "                    new_text = obj.get('content')\n",
    "                    new_text = re.sub('\\W+', ' ', new_text.lower())\n",
    "                    new_text = re.sub('[a-z]', ' ', new_text)\n",
    "                    new_words = new_text.split()\n",
    "                    words_list += new_words\n",
    "                    \n",
    "\n",
    "                    \n",
    "            for string in words_list:\n",
    "                \n",
    "                lemmatized = morph.parse(string)[0].normal_form\n",
    "                \n",
    "                if tag in dict_:\n",
    "                    if lemmatized in dict_[tag]:\n",
    "                        dict_[tag][lemmatized] += 1\n",
    "                    else:\n",
    "                        dict_[tag][lemmatized] = 1\n",
    "                else:\n",
    "                    dict_[tag] = {lemmatized : 1}\n",
    "#     return dict_\n",
    "    save_dict(dict_, URL_DOC[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Словарь: QuerryNumber <-> [DOCID_1, ..., DOCID_N]\n",
    "\n",
    "\"\"\"\n",
    "def qnum_docs():\n",
    "    dict_ = {}\n",
    "    with open('sample.technosphere.ir1.textrelevance.submission.txt') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                line = line[:-1].split(',')\n",
    "                if line[0] in dict_:\n",
    "                    dict_[line[0]] += [line[1]]\n",
    "                else:\n",
    "                    dict_[line[0]] = [line[1]]\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Словарь: DOCID <-> TAG <-> LENGTH\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_doc_tag_len():\n",
    "    doc_tag_len = {}\n",
    "    i=0\n",
    "    for docid in range(1, 38115):\n",
    "        if not i % 100:\n",
    "            clear_output(wait=True)\n",
    "            print('Files handled: {}; Files left: {}'.format(i,38114-i))\n",
    "        cur_doc = open_dict(str(docid))\n",
    "        i+=1\n",
    "        for tag in cur_doc:\n",
    "\n",
    "            for word in cur_doc[tag]:\n",
    "\n",
    "                if str(docid) in doc_tag_len:\n",
    "                    if tag in doc_tag_len[str(docid)]:\n",
    "                        doc_tag_len[str(docid)][tag] += cur_doc[tag][word]\n",
    "                    else:\n",
    "                        doc_tag_len[str(docid)][tag] = cur_doc[tag][word]\n",
    "                else:\n",
    "                    doc_tag_len[str(docid)] = {tag : cur_doc[tag][word]}\n",
    "                        \n",
    "    return doc_tag_len       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Словарь: QuerryNumber <-> TAG <-> AverageLength\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qnum_len():\n",
    "    avg = {}\n",
    "    \n",
    "    for qnum in QNUM_D:\n",
    "        \n",
    "        counter = {'body':0, 'title':0, 'h1':0, 'h2':0, 'a':0, 'meta':0}\n",
    "        \n",
    "        for docid in QNUM_D[qnum]:\n",
    "            \n",
    "            if docid in DOC_TAG_L:\n",
    "                for tag in DOC_TAG_L[docid]:\n",
    "                    counter[tag] += DOC_TAG_L[docid][tag]\n",
    "\n",
    "            \n",
    "        for key in counter:\n",
    "            counter[key] = counter[key] / len(QNUM_D[qnum])\n",
    "            \n",
    "        avg[qnum] = counter\n",
    "            \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Словарь: QuerryNumber <-> [lemmatized_1, ..., lemmatized_N]\n",
    "То есть список лемматизированных слов запроса\n",
    "\n",
    "\"\"\"\n",
    "def requests():\n",
    "    dict_ = {}\n",
    "    with open('queries.numerate.txt', encoding='utf8') as f:\n",
    "        line = f.readline().lower()\n",
    "        while line:\n",
    "            list_ = line.split()\n",
    "            lemmatized = [morph.parse(word)[0].normal_form for word in list_[1:]]\n",
    "            dict_[list_[0]] = lemmatized\n",
    "    \n",
    "            line = f.readline().lower()\n",
    "            \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 775 ms, sys: 0 ns, total: 775 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "QNUM_Q = requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 ms, sys: 18.8 ms, total: 46.4 ms\n",
      "Wall time: 41.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "QNUM_D = qnum_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files handled: 38100; Files left: 14\n",
      "CPU times: user 2min 46s, sys: 6.28 s, total: 2min 52s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DOC_TAG_L = get_doc_tag_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 9.97 ms, total: 118 ms\n",
      "Wall time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q_TAG_AVG = qnum_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Вспомогательная функция для вывода полученного решения\n",
    "\n",
    "\"\"\"\n",
    "def print_submmission(final_dict):\n",
    "    for qnum in final_dict:\n",
    "        res = list(final_dict[qnum].items())\n",
    "        res.sort(key=lambda i: i[1])\n",
    "        res = res[-10:]\n",
    "        for i in res[::-1]:\n",
    "            print(str(qnum)+','+str(i[0])+'    '+str(np.around(i[1],4))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Здесь мы применяем BM25F для оценки релевантности документа\n",
    "\n",
    "Будем применять не модель не к документу целиком, а к каждому тегу в документе\n",
    "\n",
    "Далее для каждого тега мы можем прикинуть его значимость\n",
    "(т.е. будем получать некую оценку от каждого тега, а далее учитывать ее с некоторым весом)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_score():\n",
    "    qnum_doc_tag_score = {}\n",
    "    idf = {}\n",
    "    score = {}\n",
    "    additional_docs = []\n",
    "\n",
    "    for qnum in QNUM_Q:\n",
    "        docs = QNUM_D[qnum]\n",
    "        \n",
    "        if int(qnum) < 200:\n",
    "            additional_docs = []\n",
    "            for i in [int(qnum)+1, int(qnum)+2, int(qnum)+3, int(qnum)+4]: # int(qnum)+5]:\n",
    "                additional_docs += QNUM_D[str(i)]\n",
    "                \n",
    "        elif int(qnum) >= 200:\n",
    "            additional_docs = []\n",
    "            for i in [int(qnum)-1, int(qnum)-2, int(qnum)-3, int(qnum)-4]:#, int(qnum)-5]:\n",
    "                additional_docs += QNUM_D[str(i)]\n",
    "            \n",
    "            \n",
    "        # Временныйы словарь частот слов по тегам для очередного запроса\n",
    "        tmp = {}\n",
    "        for docid in docs+additional_docs:\n",
    "            tmp[docid] = open_dict(docid)\n",
    "        \n",
    "        for word in QNUM_Q[qnum]:\n",
    "            \n",
    "            n_q_i = {'body':0, 'title':0, 'h1':0, 'h2':0, 'a':0, 'meta':0}\n",
    "            N = len(docs+additional_docs)\n",
    "            \n",
    "            # Подсчет документов-тегов, в которых есть терм\n",
    "            for docid in tmp:\n",
    "                for tag in tmp[docid]:\n",
    "                    if word in tmp[docid][tag]:\n",
    "                        n_q_i[tag] += 1\n",
    "\n",
    "            # Подсчет IDF для очередного терма запроса\n",
    "            for tag in ['body', 'title', 'h1', 'h2', 'a', 'meta']:\n",
    "                tmp_idf = np.log((N-n_q_i[tag]+0.5) / (n_q_i[tag]+0.5)) #tag -> body?\n",
    "                if tmp_idf > 0:\n",
    "                    idf[tag] = tmp_idf\n",
    "                else:\n",
    "                    idf[tag] = 0\n",
    "                    \n",
    "            #Подсчёт финального скора для каждого документа\n",
    "            for docid in docs:\n",
    "                for tag in tmp[docid]:\n",
    "                    if word in tmp[docid][tag]:\n",
    "                        \n",
    "                        b = 0.75\n",
    "                        \n",
    "                        if tag in ['title']:\n",
    "                            w = 2\n",
    "                        \n",
    "                        elif tag in ['h1', 'meta']:\n",
    "                            w = 1.5\n",
    "                            \n",
    "                        elif tag in ['body', 'h2']:\n",
    "                            w = 0.5\n",
    "                            \n",
    "                        elif tag in ['a']:\n",
    "                            w = 0.01\n",
    "                            \n",
    "                        else:\n",
    "                            w = 0\n",
    "                            \n",
    "                        TF = w*tmp[docid][tag][word]\n",
    "                        \n",
    "                        score[tag] = idf[tag]*((TF*(2+1))/\\\n",
    "                                    (TF+2*(1-b+b*(DOC_TAG_L[docid][tag]/Q_TAG_AVG[qnum][tag]))))\n",
    "                        \n",
    "                        if qnum in qnum_doc_tag_score:\n",
    "                            if docid in qnum_doc_tag_score[qnum]:\n",
    "                                if tag in qnum_doc_tag_score[qnum][docid]:\n",
    "                                    qnum_doc_tag_score[qnum][docid][tag] += score[tag]\n",
    "                                else:\n",
    "                                    qnum_doc_tag_score[qnum][docid][tag]  = score[tag]\n",
    "                            else:\n",
    "                                qnum_doc_tag_score[qnum][docid] = {tag : score[tag]}\n",
    "                        else:\n",
    "                            qnum_doc_tag_score[qnum] = {docid : {tag: score[tag]}}\n",
    "        clear_output(wait=True)\n",
    "        print('{} <-- requests ready'.format(qnum))\n",
    "    \n",
    "    return qnum_doc_tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 <-- requests ready\n",
      "CPU times: user 2min 31s, sys: 6.28 s, total: 2min 37s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL = result(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(final_dict):\n",
    "    with open('./submissions/'+'tag_a_01.txt', 'w') as f:\n",
    "        f.write('QueryId,DocumentId\\n')\n",
    "        for qnum in final_dict:\n",
    "            res = list(final_dict[qnum].items())\n",
    "            res.sort(key=lambda i: i[1])\n",
    "            res = res[-10:]\n",
    "            for i in res[::-1]:\n",
    "                print(str(qnum)+','+str(i[0])+'\\n')\n",
    "                str(qnum)+','+str(i[0])+'\\n'\n",
    "                f.write(str(qnum)+','+str(i[0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(final_dict):\n",
    "    qnum_doc_score = {}\n",
    "\n",
    "    for qnum in final_dict:\n",
    "        for doc in final_dict[qnum]:\n",
    "            for tag in final_dict[qnum][doc]:\n",
    "                \n",
    "                w = 1\n",
    "                    \n",
    "                if qnum in qnum_doc_score:\n",
    "                    if doc in qnum_doc_score[qnum]:\n",
    "                        qnum_doc_score[qnum][doc] += w*final_dict[qnum][doc][tag]\n",
    "                    else:\n",
    "                        qnum_doc_score[qnum][doc] = w*final_dict[qnum][doc][tag]\n",
    "                else:\n",
    "                    qnum_doc_score[qnum] = {doc : w*final_dict[qnum][doc][tag]}\n",
    "\n",
    "    return qnum_doc_score            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_submission(FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Далее идут функции, реализующие SpellChecker.\n",
    "Составляем словарь из всех слов во всех документах\n",
    "Составляем словари биграмм по сотням, т.к. память не резиновая :)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_freq_dict():\n",
    "    public_d = {}\n",
    "    i=0\n",
    "    for i in range(1, 38115):\n",
    "        if not i%100:\n",
    "            print('{} <--- loaded'.format(i))\n",
    "        docid = str(i)\n",
    "        curr_dict = open_public(docid)\n",
    "        for tag in curr_dict:\n",
    "            for word in curr_dict[tag]:\n",
    "                if word and len(word)<18:\n",
    "                    if word[0]>='a' and word[0]<='z' or word[0]>='а' and word[0]<='я' or word[0]>='0' and word[0]<='9':\n",
    "                        if word in public_d:\n",
    "                            public_d[word] += curr_dict[tag][word]\n",
    "                        else:\n",
    "                            public_d[word] = curr_dict[tag][word]\n",
    "                    \n",
    "    with open('./freq_dicts/public.txt', 'w') as f:\n",
    "        for word in public_d:\n",
    "            if public_d[word] > 3:\n",
    "                f.write(word+' '+str(public_d[word])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_bigr_dict():\n",
    "    public_d={}\n",
    "    i=0\n",
    "    for qnum in QNUM_D:\n",
    "\n",
    "        for docid in QNUM_D[qnum]:\n",
    "            docid = str(docid)\n",
    "            curr_dict = open_bigramm(docid)\n",
    "            for tag in curr_dict:\n",
    "                for bigramm in curr_dict[tag]:\n",
    "                    tmp = bigramm.split()\n",
    "                    if len(tmp[0])<18 and len(tmp[1])<18:\n",
    "                        if tmp[0][0]>='a' and tmp[0][0]<='z' or tmp[0][0]>='а' and tmp[0][0]<='я':\n",
    "                            if tmp[1][0]>='a' and tmp[1][0]<='z' or tmp[1][0]>='а' and tmp[1][0]<='я':\n",
    "                                if bigramm in public_d:\n",
    "                                    public_d[bigramm] += curr_dict[tag][bigramm]\n",
    "                                else:\n",
    "                                    public_d[bigramm] = curr_dict[tag][bigramm]\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Querry {} is ready'.format(qnum))\n",
    "        if int(qnum) in {100, 200, 300, 399}:\n",
    "            with open('./freq_dicts/bigramms_dict'+qnum+'.txt', 'w') as f:\n",
    "                for bigramm in public_d:\n",
    "                    if public_d[bigramm] > 1:\n",
    "                        f.write(bigramm+' '+str(public_d[bigramm])+'\\n')\n",
    "            public_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Здесь реализация split, join, transletiration, orphography\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def correct(s):\n",
    "    final_s = ''\n",
    "    s = sym_spell.word_segmentation(s) \n",
    "    s = str(s[0])\n",
    "    l = s.split()\n",
    "    tmp_s = ''\n",
    "    f = True\n",
    "    ended = True\n",
    "    for i in range(len(l[:-1])):\n",
    "        if f:\n",
    "            suggestions = sym_spell.lookup(l[i]+l[i+1], Verbosity.ALL,\n",
    "                                max_edit_distance=2, include_unknown=True)\n",
    "            if suggestions:\n",
    "                if int(str((suggestions[0])).split(', ')[2]) > 200 and int(str((suggestions[0])).split(', ')[1]) == 0:\n",
    "                    tmp_s += str(suggestions[0]).split(', ')[0] + ' '\n",
    "                    f = False\n",
    "                    if i+1 == len(l)-1:\n",
    "                        ended = False\n",
    "                else:\n",
    "                    tmp_s += l[i]+' '\n",
    "        else:\n",
    "            f = True\n",
    "    if ended:\n",
    "        tmp_s += l[-1]\n",
    "    \n",
    "    s = tmp_s\n",
    "        \n",
    "    for i in s.split():\n",
    "        suggestions = sym_spell.lookup(i, Verbosity.ALL,\n",
    "                                max_edit_distance=2, include_unknown=True)\n",
    "        res = []\n",
    "        for suggestion in suggestions:\n",
    "            res += [str(suggestion).split(', ')]\n",
    "        res.sort(key=lambda i: 0.3*np.log(int(i[2])) - int(i[1]))\n",
    "        final_s += res[::-1][0][0] + ' '\n",
    "        \n",
    "    return final_s[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transletiration(s):\n",
    "    eng_rus = {'q':'й', 'w':'ц', 'e':'у', 'r':'к', 't':'е', 'y':'н'\\\n",
    "           , 'u':'г', 'i':'ш', 'o':'щ', 'p':'з','[':'х',']':'ъ','a':'ф',\\\n",
    "           's':'ы','d':'в','f':'а','g':'п','h':'р','j':'о','k':'л'\\\n",
    "           ,'l':'д',';':'ж','\\'':'э','z':'я','x':'ч','c':'с','v':'м','b':'и','n':'т','m':'ь',',':'б','.':'ю'}\n",
    "    res = ''\n",
    "    l = s.split()\n",
    "    for i in l:\n",
    "        for j in i:\n",
    "            res += eng_rus[j]\n",
    "        res += ' '\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPELLCHECKER -> {TRANSLITERATION, SPLIT, JOIN} -> ORPHOGRAPHY\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "# sym_spell.load_dictionary('./freq_dicts/public.txt', term_index=0, count_index=1)\n",
    "\n",
    "def get_newQ():\n",
    "    newQ={}\n",
    "\n",
    "    with open('queries.numerate.txt','r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            l = line.split()\n",
    "            clear_output(wait=True)\n",
    "            print(l[0])\n",
    "            s = ''\n",
    "            for i in l[1:]:\n",
    "                s += i + ' '\n",
    "            s = s[:-1]\n",
    "            \n",
    "            if l[1][0]>='a' and l[1][0] <= 'z':\n",
    "                s = make_transletiration(s)\n",
    "                \n",
    "            elif l[2:]:\n",
    "                if l[2][0]>='a' and l[2][0]<='z' and l[-1][0]>='a' and l[-1][0]<='z' :\n",
    "                    s = ''\n",
    "                    for i in l[2:]:\n",
    "                        s += i + ' '\n",
    "                    s = s[:-1]\n",
    "                    s = l[1] + ' ' + make_transletiration(s)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            s = correct(s)\n",
    "            \n",
    "            if l[0] in {'1','100','200','300'}: \n",
    "                sym_spell.load_bigram_dictionary('./freq_dicts/bigramms_dict'+l[0]+'.txt', term_index=0, count_index=2)\n",
    "\n",
    "            suggestions = sym_spell.lookup_compound(s, max_edit_distance=2)\n",
    "            res = str(suggestions[0]).split(', ')\n",
    "            newQ[l[0]] = res[0]\n",
    "            line = f.readline()\n",
    "            \n",
    "    with open('queries.numerate.new.txt','w') as f:\n",
    "        for q in newQ:\n",
    "            f.write(q+'\\t'+newQ[q]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подводя итог:** \n",
    "\n",
    "- Удалось добиться достаточно хорошего результата, используя классическую модель BM25F\n",
    "- Подбирать веса без train - проблематично, но +- правильно подобранные веса очень даже неплохо прибавляют \n",
    "- Больше всего скор вырос после добавления лемматизации. Пробовал стемминг, но вышло хуже (мб сам где ошибся)\n",
    "- Очень неплохо подбросил spellchecker\n",
    "- Ещё была идея пройтись по документам антиспамом и понижать, если антиспам сработал, но пока что не успел"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
