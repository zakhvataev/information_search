{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import simpleneighbors\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "import _pickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pymorphy2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Парсинг практически совпадает с тем, что было в ноутбуке с BM25F\n",
    "\"\"\"\n",
    "\n",
    "def url_doc():\n",
    "    with open ('urls.numerate.txt') as f:\n",
    "        line = True\n",
    "        u_d = {}\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                id_url = line.split()\n",
    "                u_d[id_url[1]] = id_url[0]\n",
    "    return u_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files():\n",
    "    folders = ['./content/content/20170702/', './content/content/20170704/', './content/content/20170707/',\\\n",
    "               './content/content/20170708/', './content/content/20170710/', './content/content/20170711/',\\\n",
    "               './content/content/20170717/', './content/content/20170726/']\n",
    "    i = 0\n",
    "    for cur_folder in folders:\n",
    "        for file in os.listdir(cur_folder):\n",
    "            parser(cur_folder+file)\n",
    "            clear_output(wait=True)\n",
    "            print('Loaded {} files, {} to go'.format(i, 38114 - i))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(way):\n",
    "    \n",
    "    with open(way, encoding='utf-8', errors='ignore') as file:\n",
    "        url = file.readline()[:-1]\n",
    "        \n",
    "        raw_html = file.read()\n",
    "        soup = BeautifulSoup(raw_html, 'html')\n",
    "        \n",
    "        text = ''\n",
    "        text_lemma = ''\n",
    "        \n",
    "        for obj in soup.find_all(['title']):\n",
    "            \n",
    "            text = obj.text\n",
    "            \n",
    "            words_list = text.split()\n",
    "            \n",
    "            for string in words_list:\n",
    "                text_lemma += string + ' '\n",
    "                \n",
    "            text_lemma = text_lemma[:-1]\n",
    "        \n",
    "        tuple_ = (text_lemma, len(text_lemma))  \n",
    "                    \n",
    "    save_tuple(tuple_, URL_DOC[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tuple(tuple_, docid):\n",
    "    with open('./TuplesForNN/'+docid+'.pkl', 'wb') as out:\n",
    "        pickle.dump(tuple_, out, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tuple(docid):\n",
    "    with open('./TuplesForNN/'+docid+'.pkl', 'rb') as inp:\n",
    "        tuple_ = pickle.load(inp)\n",
    "    return tuple_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests():\n",
    "    dict_ = {}\n",
    "    text = ''\n",
    "    with open('Q_NUMERATE.txt', encoding='utf8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            list_ = line.split()\n",
    "            for word in list_[1:]:\n",
    "                text += word + ' '\n",
    "            text = text[:-1]\n",
    "            \n",
    "            dict_[list_[0]] = text\n",
    "    \n",
    "            line = f.readline()\n",
    "            text = '' \n",
    "    \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qnum_docs():\n",
    "    dict_ = {}\n",
    "    with open('sample.technosphere.ir1.textrelevance.submission.txt') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                line = line[:-1].split(',')\n",
    "                if line[0] in dict_:\n",
    "                    dict_[line[0]] += [line[1]]\n",
    "                else:\n",
    "                    dict_[line[0]] = [line[1]]\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_docid():\n",
    "    t_d = {}\n",
    "    for qnum in QNUM_D:\n",
    "        \n",
    "        for docid in QNUM_D[qnum]:\n",
    "            title, len_ = open_tuple(docid)\n",
    "            t_d[title[:150]] = docid\n",
    "        clear_output(wait=True)\n",
    "        print('{} querries handled'.format(qnum))\n",
    "    return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(qnum):\n",
    "    \n",
    "    list_docs = QNUM_D[qnum]\n",
    "    title_list = []\n",
    "    \n",
    "    for doc in list_docs:\n",
    "        title_len = open_tuple(doc)\n",
    "\n",
    "        text = title_len[0]\n",
    "        text = re.sub('\\W+', ' ', text.lower())\n",
    "        \n",
    "        if title_len[0] != '':\n",
    "            title_list += [text[:150]]\n",
    "\n",
    "    encodings = model.signatures['response_encoder'](input=tf.constant(title_list),\n",
    "                                                     context=tf.constant(title_list))\n",
    "\n",
    "    index = simpleneighbors.SimpleNeighbors(len(encodings['outputs'][0]), metric='angular')\n",
    "\n",
    "    for ind, title in enumerate(title_list):\n",
    "        index.add_one(title, encodings['outputs'][ind])\n",
    "\n",
    "    index.build()\n",
    "            \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinalRound(_continue=1):\n",
    "    \n",
    "    for qnum in QNUM_Q:\n",
    "        if int(qnum) >= _continue:\n",
    "            index = get_index(qnum)\n",
    "            query_embedding = model.signatures['question_encoder']\\\n",
    "                                (tf.constant([QNUM_Q[qnum]]))['outputs'][0]\n",
    "            nearest_10 = index.nearest(query_embedding, n=10)\n",
    "            with open('./submissions/NN_3.txt', 'a') as f:\n",
    "                if qnum =='1':\n",
    "                    f.write('QueryId,DocumentId\\n')\n",
    "\n",
    "                for title in nearest_10:\n",
    "                    f.write(str(qnum)+','+str(TITLE_DOC[title])+'\\n')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('{} queries handled'.format(qnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_start():\n",
    "    def printer(r):\n",
    "        clear_output(wait=True)\n",
    "        print('URL_DOC____{}'.format(r[0]))\n",
    "        print('model______{}'.format(r[1]))\n",
    "        print('QNUM_Q_____{}'.format(r[2]))\n",
    "        print('QNUM_D_____{}'.format(r[3]))\n",
    "        print('TITLE_DOC__{}'.format(r[4]))\n",
    "        \n",
    "    global URL_DOC, model, QNUM_Q, QNUM_D, TITLE_DOC\n",
    "    printer(['X','X','X','X','X'])\n",
    "    URL_DOC = url_doc()\n",
    "    printer(['V','X','X','X','X'])\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\")\n",
    "    printer(['V','V','X','X','X'])\n",
    "    QNUM_Q = requests()\n",
    "    printer(['V','V','V','X','X'])\n",
    "    QNUM_D = qnum_docs()\n",
    "    printer(['V','V','V','V','X'])\n",
    "    TITLE_DOC = title_docid()\n",
    "    printer(['V','V','V','V','V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Создаем папку с кортежами в виде: ./TuplesForNN/doc_id.pkl в которых лежат тайтлы\"\"\"\n",
    "open_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model______V\n",
      "QNUM_Q_____V\n",
      "QNUM_D_____V\n",
      "TITLE_DOC__V\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Загрузка базовых словарей\"\"\"\n",
    "on_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 queries handled\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Загрузка сабмита\"\"\"\n",
    "FinalRound()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
